{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, Add\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.applications import VGG19\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n",
      "[240 320]\n"
     ]
    }
   ],
   "source": [
    "ratio = 2\n",
    "channels = 3\n",
    "HR_size = np.array([480,640])#\n",
    "LR_size = HR_size // ratio\n",
    "H_h = HR_size[0]\n",
    "H_w = HR_size[1]\n",
    "L_h = LR_size[0]\n",
    "L_w = LR_size[1]\n",
    "print(HR_size[0])\n",
    "print(LR_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "#         self.ratio = ratio\n",
    "#         self.HR_size = HR_size\n",
    "#         self.LR_size = HR_size / ratio\n",
    "#         self.H_h = HR_size[0]\n",
    "#         self.H_w = HR_size[1]\n",
    "#         self.L_h = LR_size[0]\n",
    "#         self.L_h = LR_size[1]\n",
    "        \n",
    "    \n",
    "    \n",
    "    def load_data(self, batch_size = 1):\n",
    "        files = glob.glob(\"../../images/train/*.png\", recursive=True)\n",
    "        batch_images = random.sample(files, batch_size)\n",
    "    \n",
    "        HR_images = []\n",
    "        LR_images = []\n",
    "    \n",
    "        for img_path in batch_images:\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            HR_image = img.resize((H_w, H_h))  #(64, 64)\n",
    "            LR_image = img.resize((L_w, L_h))\n",
    "            HR_image = np.array(HR_image)\n",
    "            #img_hr = (img_hr - 127.5) / 127.5\n",
    "            LR_image = np.array(LR_image)\n",
    "            #img_lr = (img_lr - 127.5) / 127.5\n",
    "\n",
    "#             if not is_testing and np.random.random() < 0.5:\n",
    "#                 img_hr = np.fliplr(img_hr)\n",
    "#                 img_lr = np.fliplr(img_lr)\n",
    "\n",
    "            HR_images.append(HR_image)\n",
    "            LR_images.append(LR_image)\n",
    "        \n",
    "        HR_images = np.array(HR_images) / 127.5 - 1\n",
    "        LR_images = np.array(LR_images) / 127.5 - 1\n",
    "        \n",
    "        return HR_images, LR_images\n",
    "\n",
    "#------------------------------------------------------------------#\n",
    "  \n",
    "#------------------------------------------------------------------#\n",
    "\n",
    "class predDataLoader():          \n",
    "    def load_data(self, batch_size, counter):\n",
    "        random.seed(counter)\n",
    "        np.random.seed(counter)\n",
    "\n",
    "        files = glob.glob(\"../../images/test/*.png\", recursive=True)\n",
    "        batch_images = random.sample(files, batch_size)\n",
    "\n",
    "        imgs_or = []\n",
    "        for img_path in batch_images:\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            img_or = np.array(img)\n",
    "            imgs_or.append(img_or)\n",
    "\n",
    "        imgs_or = np.array(imgs_or) / 127.5 - 1.\n",
    "\n",
    "        return imgs_or\n",
    "    \n",
    "    \n",
    "#------------------------------------------------------------------#\n",
    "  \n",
    "#------------------------------------------------------------------#\n",
    "\n",
    "class SRGAN():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Input shape\n",
    "#         self.channels = 3\n",
    "#         self.lr_height = 288                 # Low resolution height\n",
    "#         self.lr_width = 384                  # Low resolution width\n",
    "#         self.lr_shape = (self.lr_height, self.lr_width, self.channels)\n",
    "#         resLevel = 2 #\n",
    "#         self.hr_height = self.lr_height*resLevel  # High resolution height\n",
    "#         self.hr_width = self.lr_width*resLevel     # High resolution width\n",
    "#         self.hr_shape = (self.hr_height, self.hr_width, self.channels)\n",
    "\n",
    "        #残差ブロックの数\n",
    "        # Number of residual blocks in the generator\n",
    "        self.n_residual_blocks = 16 #\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        #img_hrの特徴量をVGG19で算出する\n",
    "        # We use a pre-trained VGG19 model to extract image features from the high resolution\n",
    "        # and the generated high resolution images and minimize the mse between them\n",
    "        self.VGG = self.build_vgg()\n",
    "        self.VGG.trainable = False\n",
    "        self.VGG.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "        #データはここで読み込まれる\n",
    "        # Configure data loader\n",
    "        #self.dataset_name = 'img_align_celeba'\n",
    "        self.data_loader = DataLoader()\n",
    "        self.pred_data_loader = predDataLoader()\n",
    "\n",
    "        #Dのサイズ\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patchH = int(H_h / 2**4) #\n",
    "        patchW = int(H_w / 2**4) #\n",
    "        self.disc_patch = (patchH, patchW, 1) \n",
    "\n",
    "        #DとGのチャンネル設定\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 60 #gf\n",
    "        self.df = 60 #df\n",
    "        #Dビルドとコンパイル\n",
    "        # Build and compile the discriminator\n",
    "        self.Discriminator = self.build_discriminator()\n",
    "        self.Discriminator.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        #Gのビルド\n",
    "        # Build the generator\n",
    "        self.Generator = self.build_generator()\n",
    "\n",
    "        # High res. and low res. images\n",
    "        HR_image = Input(shape = (H_h, H_w, channels))\n",
    "        LR_image = Input(shape = (L_h, H_w, channels))\n",
    "\n",
    "        #Gで生成されたhrのimg\n",
    "        # Generate high res. version from low res.\n",
    "        SR_image = self.Generator(LR_image)\n",
    "\n",
    "        #hrのimgの特徴量の算出\n",
    "        # Extract image features of the generated img\n",
    "        SR_features = self.VGG(SR_image)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.Discriminator.trainable = False\n",
    "\n",
    "        #学習モデルコンパイル\n",
    "        # Discriminator determines validity of generated high res. images\n",
    "        validity = self.Discriminator(SR_image)\n",
    "\n",
    "        self.combined = Model([LR_image, HR_image], [validity, SR_features])\n",
    "        self.combined.compile(loss=['binary_crossentropy', 'mse'],\n",
    "                              loss_weights=[1e-3, 1],\n",
    "                              optimizer=optimizer)\n",
    "        \n",
    "    #------------------------------------------------------------------#\n",
    "    \n",
    "    def build_vgg(self):\n",
    "        \"\"\"\n",
    "        Builds a pre-trained VGG19 model that outputs image features extracted at the\n",
    "        third block of the model\n",
    "        \"\"\"\n",
    "        VGG = VGG19(weights=\"imagenet\")\n",
    "        # Set outputs to outputs of last conv. layer in block 3\n",
    "        # See architecture at: https://github.com/keras-team/keras/blob/master/keras/applications/vgg19.py\n",
    "        VGG.outputs = [VGG.layers[9].output]\n",
    "\n",
    "        img = Input(shape = (H_h, H_w, channels))\n",
    "\n",
    "        # Extract image features\n",
    "        img_features = VGG(img)\n",
    "\n",
    "        return Model(img, img_features)\n",
    "    \n",
    "    #------------------------------------------------------------------#\n",
    "    \n",
    "    #Generator の実装\n",
    "    def build_generator(self):\n",
    "\n",
    "        #残差ブロックの中身\n",
    "        def residual_block(layer_input, n_filters):\n",
    "            \"\"\"Residual block described in paper\"\"\"\n",
    "            d = Conv2D(n_filters, kernel_size=3, strides=1, padding='same')(layer_input)\n",
    "            d = Activation('relu')(d)\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Conv2D(n_filters, kernel_size=3, strides=1, padding='same')(d)\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Add()([d, layer_input])\n",
    "            return d\n",
    "\n",
    "        #解像度を2倍にするUpSampling\n",
    "        def deconv2d(layer_input):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input) #\n",
    "            u = Conv2D(120, kernel_size=3, strides=1, padding='same')(u) #\n",
    "            u = Activation('relu')(u)\n",
    "            return u\n",
    "\n",
    "        # Low resolution image input\n",
    "        img_lr = Input(shape = (L_h, L_w, channels))\n",
    "\n",
    "        # Pre-residual block\n",
    "        c1 = Conv2D(60, kernel_size=9, strides=1, padding='same')(img_lr) #\n",
    "        c1 = Activation('relu')(c1)\n",
    "\n",
    "        # Propogate through residual blocks\n",
    "        r = residual_block(c1, self.gf)\n",
    "        for _ in range(self.n_residual_blocks - 1):\n",
    "            r = residual_block(r, self.gf)\n",
    "\n",
    "        #去の残差ブロックと組み合わせる\n",
    "        # Post-residual block\n",
    "        c2 = Conv2D(60, kernel_size=3, strides=1, padding='same')(r) #\n",
    "        c2 = BatchNormalization(momentum=0.8)(c2)\n",
    "        c2 = Add()([c2, c1])\n",
    "\n",
    "        # Upsampling\n",
    "        u2 = deconv2d(c2)\n",
    "        #u2 = deconv2d(u1)\n",
    "\n",
    "        # Generate high resolution output\n",
    "        gen_hr = Conv2D(channels, kernel_size=9, strides=1, padding='same', activation='tanh')(u2)\n",
    "\n",
    "        return Model(img_lr, gen_hr)\n",
    "    \n",
    "    \n",
    "    #------------------------------------------------------------------#\n",
    "    \n",
    "    #Discriminator の実装\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        \n",
    "        def d_block(layer_input, filters, strides=1, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=3, strides=strides, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        # Input img\n",
    "        d0 = Input(shape = (H_h, H_w, channels))\n",
    "        #畳み込み層、チャンネル数を最終的に16倍に\n",
    "        d1 = d_block(d0, self.df, bn=False)\n",
    "        d2 = d_block(d1, self.df, strides=2)\n",
    "        d3 = d_block(d2, self.df*2)\n",
    "        d4 = d_block(d3, self.df*2, strides=2)\n",
    "        d5 = d_block(d4, self.df*4)\n",
    "        d6 = d_block(d5, self.df*4, strides=2)\n",
    "        d7 = d_block(d6, self.df*8)\n",
    "        d8 = d_block(d7, self.df*8, strides=2)\n",
    "        #この時点で画像サイズ1/16\n",
    "        d9 = Dense(self.df*16)(d8)\n",
    "        d10 = LeakyReLU(alpha=0.2)(d9)\n",
    "        validity = Dense(1, activation='sigmoid')(d10)\n",
    "\n",
    "        return Model(d0, validity)\n",
    "    \n",
    "    \n",
    "    #------------------------------------------------------------------#\n",
    "    \n",
    "    \n",
    "    #学習\n",
    "    def train(self, epochs, batch_size, sample_interval=100):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "        psnr_file = open('psnr.csv' , 'w+')\n",
    "        psnr_file.close()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ----------------------\n",
    "            #  Train Discriminator\n",
    "            # ----------------------\n",
    "\n",
    "            # Sample images and their conditioning counterparts\n",
    "            HR_images, LR_images = self.data_loader.load_data(batch_size)\n",
    "\n",
    "            # From low res. image generate high res. version\n",
    "            SR_images = self.Generator.predict(LR_images)\n",
    "\n",
    "            valid = np.ones((batch_size,) + self.disc_patch)\n",
    "            fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "\n",
    "            #Dのloss\n",
    "            # Train the discriminators (original images = real / generated = Fake)\n",
    "            d_loss_real = self.Discriminator.train_on_batch(HR_images, valid)\n",
    "            d_loss_fake = self.Discriminator.train_on_batch(SR_images, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generator\n",
    "            # ------------------\n",
    "\n",
    "            # Sample images and their conditioning counterparts\n",
    "            #これいる？\n",
    "            HR_images, LR_images = self.data_loader.load_data(batch_size)\n",
    "\n",
    "            # The generators want the discriminators to label the generated images as real\n",
    "            valid = np.ones((batch_size,) + self.disc_patch)\n",
    "\n",
    "            # Extract ground truth image features using pre-trained VGG19 model\n",
    "            image_features = self.VGG.predict(HR_images)\n",
    "\n",
    "            # Train the generators\n",
    "            g_loss = self.combined.train_on_batch([LR_images, HR_images], [valid, image_features])\n",
    "\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "            # Plot the progress\n",
    "            print (\"%d time: %s\" % (epoch, elapsed_time))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "                self.Generator.save_weights('srg_weight.h5')\n",
    "                \n",
    "                \n",
    "    #------------------------------------------------------------------#\n",
    "    \n",
    "            \n",
    "    def sample_images(self, epoch):\n",
    "        def denormalize(input_data):\n",
    "            input_data = (input_data + 1) * 127.5\n",
    "            return input_data.astype(np.uint8)\n",
    "        os.makedirs('../../images/images/%s' % self.dataset_name, exist_ok=True)\n",
    "        r, c = 2, 2\n",
    "\n",
    "        HR_images, LR_images = self.data_loader.load_data(batch_size=2)\n",
    "        SR_images = self.Generator.predict(LR_images)\n",
    "\n",
    "        LR_images = denormalize(LR_images)\n",
    "        SR_images = denormalize(SR_images)\n",
    "        HR_images = denormalize(HR_images)\n",
    "\n",
    "        def psnr_calc(img1: np.ndarray, img2: np.ndarray, upscaling=2):\n",
    "            def convert(img):\n",
    "                return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            def extract_y(image: np.ndarray) -> np.ndarray:\n",
    "                if image.ndim == 2:\n",
    "                    return image\n",
    "                image = image.astype(np.int32)\n",
    "                return ((image[:, :, 2] * 65.481 / 255.\n",
    "                          + image[:, :, 1] * 128.553 / 255.\n",
    "                          + image[:, :, 0] * 24.966 / 255.) + 16).astype(np.int32)\n",
    "\n",
    "\n",
    "            def psnr(img1, img2):\n",
    "                mse = np.mean((img1 - img2) ** 2)\n",
    "                if mse == 0:\n",
    "                    return 100\n",
    "                PIXEL_MAX = 255.0\n",
    "                return 10 * math.log10(PIXEL_MAX * PIXEL_MAX / mse)\n",
    "  \n",
    "            img1_conv=convert(img1)\n",
    "            img2_conv=convert(img2)\n",
    "  \n",
    "            # BGR -> YCrCb\n",
    "            # 画像はcv2.imreadで読まれている前提 [0, 255]\n",
    "            y1 = extract_y(img1_conv)\n",
    "            y2 = extract_y(img2_conv)\n",
    "            # 周囲のcropping\n",
    "            # assert y1.shape == y2.shape\n",
    "            h, w = y1.shape\n",
    "            cr = upscaling\n",
    "            cropped_y1 = y1[cr:h - cr, cr:w - cr].astype(np.float64)\n",
    "            cropped_y2 = y2[cr:h - cr, cr:w - cr].astype(np.float64)\n",
    "\n",
    "            # psnr\n",
    "            psnr_val = psnr(cropped_y1, cropped_y2)\n",
    "            return psnr_val\n",
    "        \n",
    "        \n",
    "        # Save generated images and the high resolution originals\n",
    "        titles = ['Generated', 'Original']\n",
    "\n",
    "        cv2.imwrite(\"../../images/images/hreal_img_0.png\",HR_images[0])\n",
    "        cv2.imwrite(\"../../images/images/hreal_img_1.png\",HR_images[1])\n",
    "        cv2.imwrite(\"../../images/images/lreal_img_0.png\",LR_images[0])\n",
    "        cv2.imwrite(\"../../images/images/lreal_img_1.png\",LR_images[1])\n",
    "        # size=(480,480)\n",
    "\n",
    "        for i in range(r):\n",
    "            psnr = psnr_calc(HR_images[i],SR_images[i])\n",
    "            print(psnr)\n",
    "            data =[epoch,psnr]\n",
    "            psnr_file = open('psnr.csv' , 'a')\n",
    "            writer = csv.writer(psnr_file, lineterminator='\\n')  \n",
    "            writer.writerow(data)\n",
    "            psnr_file.close()\n",
    "            cv2.imwrite(\"../../images/images/{}_{}img_pred.png\".format(epoch,i),SR_images[i])\n",
    "            \n",
    "            \n",
    "    #------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: expected input_18 to have shape (None, 240, 640, 3) but got array with shape (1, 240, 320, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b77b08e902e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mgan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSRGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-bfb1746ada03>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[1;31m# Train the generators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m             \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mLR_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHR_images\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1613\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1614\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1615\u001b[1;33m             check_batch_axis=True)\n\u001b[0m\u001b[0;32m   1616\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1617\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1294\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m                                     exception_prefix='model input')\n\u001b[0m\u001b[0;32m   1297\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1298\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    131\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m                             str(array.shape))\n\u001b[0m\u001b[0;32m    134\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: expected input_18 to have shape (None, 240, 640, 3) but got array with shape (1, 240, 320, 3)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = SRGAN()\n",
    "    gan.train(epochs=500, batch_size=1, sample_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
