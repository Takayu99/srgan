{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\robotics\\anaconda3\\envs\\faster_rcnn\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, Add\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.applications import VGG19\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n",
      "[60 80]\n"
     ]
    }
   ],
   "source": [
    "ratio = 8\n",
    "channels = 3\n",
    "HR_size = np.array([480,640])#\n",
    "LR_size = HR_size // ratio\n",
    "H_h = HR_size[0]\n",
    "H_w = HR_size[1]\n",
    "L_h = LR_size[0]\n",
    "L_h = LR_size[1]\n",
    "print(HR_size[0])\n",
    "print(LR_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "#         self.ratio = ratio\n",
    "#         self.HR_size = HR_size\n",
    "#         self.LR_size = HR_size / ratio\n",
    "#         self.H_h = HR_size[0]\n",
    "#         self.H_w = HR_size[1]\n",
    "#         self.L_h = LR_size[0]\n",
    "#         self.L_h = LR_size[1]\n",
    "        \n",
    "    \n",
    "    \n",
    "    def load_data(self, batch_size = 1):\n",
    "        files = glob.glob(\"../../images/train/*.png\", recursive=True)\n",
    "        batch_images = random.sample(files, batch_size)\n",
    "    \n",
    "        HR_images = []\n",
    "        LR_images = []\n",
    "    \n",
    "        for img_path in batch_images:\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            HR_image = img.resize(H_w, H_h)  #(64, 64)\n",
    "            HR_image = img.resize(L_w, L_h)\n",
    "            HR_image = np.array(HR_image)\n",
    "            #img_hr = (img_hr - 127.5) / 127.5\n",
    "            HR_image = np.array(HR_image)\n",
    "            #img_lr = (img_lr - 127.5) / 127.5\n",
    "\n",
    "#             if not is_testing and np.random.random() < 0.5:\n",
    "#                 img_hr = np.fliplr(img_hr)\n",
    "#                 img_lr = np.fliplr(img_lr)\n",
    "\n",
    "            HR_images.append(HR_image)\n",
    "            LR_images.append(LR_image)\n",
    "        \n",
    "        HR_images = HR_images / 127.5 - 1\n",
    "        LR_images = LR_images / 127.5 - 1\n",
    "        \n",
    "        return HR_images, LR_images\n",
    "\n",
    "#------------------------------------------------------------------#\n",
    "  \n",
    "#------------------------------------------------------------------#\n",
    "\n",
    "class predDataLoader():          \n",
    "    def load_data(self, batch_size, counter):\n",
    "        random.seed(counter)\n",
    "        np.random.seed(counter)\n",
    "\n",
    "        files = glob.glob(\"../../images/test/*.png\", recursive=True)\n",
    "        batch_images = random.sample(files, batch_size)\n",
    "\n",
    "        imgs_or = []\n",
    "        for img_path in batch_images:\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            img_or = np.array(img)\n",
    "            imgs_or.append(img_or)\n",
    "\n",
    "        imgs_or = np.array(imgs_or) / 127.5 - 1.\n",
    "\n",
    "        return imgs_or\n",
    "    \n",
    "    \n",
    "#------------------------------------------------------------------#\n",
    "  \n",
    "#------------------------------------------------------------------#\n",
    "\n",
    "class SRGAN():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Input shape\n",
    "#         self.channels = 3\n",
    "#         self.lr_height = 288                 # Low resolution height\n",
    "#         self.lr_width = 384                  # Low resolution width\n",
    "#         self.lr_shape = (self.lr_height, self.lr_width, self.channels)\n",
    "#         resLevel = 2 #\n",
    "#         self.hr_height = self.lr_height*resLevel  # High resolution height\n",
    "#         self.hr_width = self.lr_width*resLevel     # High resolution width\n",
    "#         self.hr_shape = (self.hr_height, self.hr_width, self.channels)\n",
    "\n",
    "        #残差ブロックの数\n",
    "        # Number of residual blocks in the generator\n",
    "        self.n_residual_blocks = 16 #\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        #img_hrの特徴量をVGG19で算出する\n",
    "        # We use a pre-trained VGG19 model to extract image features from the high resolution\n",
    "        # and the generated high resolution images and minimize the mse between them\n",
    "        self.VGG = self.build_vgg()\n",
    "        self.VGG.trainable = False\n",
    "        self.VGG.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "        #データはここで読み込まれる\n",
    "        # Configure data loader\n",
    "        #self.dataset_name = 'img_align_celeba'\n",
    "        self.data_loader = DataLoader()\n",
    "        self.pred_data_loader = predDataLoader()\n",
    "\n",
    "        #Dのサイズ\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patchH = int(H_h / 2**4) #\n",
    "        patchW = int(H_w / 2**4) #\n",
    "        self.disc_patch = (patchH, patchW, 1) \n",
    "\n",
    "        #DとGのチャンネル設定\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 60 #gf\n",
    "        self.df = 60 #df\n",
    "        #Dビルドとコンパイル\n",
    "        # Build and compile the discriminator\n",
    "        self.Discriminator = self.build_discriminator()\n",
    "        self.Discriminator.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        #Gのビルド\n",
    "        # Build the generator\n",
    "        self.Generator = self.build_generator()\n",
    "\n",
    "        # High res. and low res. images\n",
    "        HR_image = Input(shape = (H_h, H_w, channels))\n",
    "        LR_image = Input(shape = (L_h, H_w, channels))\n",
    "\n",
    "        #Gで生成されたhrのimg\n",
    "        # Generate high res. version from low res.\n",
    "        SR_image = self.Generator(LR_image)\n",
    "\n",
    "        #hrのimgの特徴量の算出\n",
    "        # Extract image features of the generated img\n",
    "        SR_features = self.VGG(SR_image)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.Discriminator.trainable = False\n",
    "\n",
    "        #学習モデルコンパイル\n",
    "        # Discriminator determines validity of generated high res. images\n",
    "        validity = self.Discriminator(SR_image)\n",
    "\n",
    "        self.combined = Model([LR_image, HR_image], [validity, SR_features])\n",
    "        self.combined.compile(loss=['binary_crossentropy', 'mse'],\n",
    "                              loss_weights=[1e-3, 1],\n",
    "                              optimizer=optimizer)\n",
    "        \n",
    "    #------------------------------------------------------------------#\n",
    "    \n",
    "    def build_vgg(self):\n",
    "        \"\"\"\n",
    "        Builds a pre-trained VGG19 model that outputs image features extracted at the\n",
    "        third block of the model\n",
    "        \"\"\"\n",
    "        VGG = VGG19(weights=\"imagenet\")\n",
    "        # Set outputs to outputs of last conv. layer in block 3\n",
    "        # See architecture at: https://github.com/keras-team/keras/blob/master/keras/applications/vgg19.py\n",
    "        VGG.outputs = [VGG.layers[9].output]\n",
    "\n",
    "        img = Input(shape = (H_h, H_w, channels))\n",
    "\n",
    "        # Extract image features\n",
    "        img_features = VGG(img)\n",
    "\n",
    "        return Model(img, img_features)\n",
    "    \n",
    "    #------------------------------------------------------------------#\n",
    "    \n",
    "    #Gの中身\n",
    "    def build_generator(self):\n",
    "\n",
    "        #残差ブロックの中身\n",
    "        def residual_block(layer_input, n_filters):\n",
    "            \"\"\"Residual block described in paper\"\"\"\n",
    "            d = Conv2D(n_filters, kernel_size=3, strides=1, padding='same')(layer_input)\n",
    "            d = Activation('relu')(d)\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Conv2D(n_filters, kernel_size=3, strides=1, padding='same')(d)\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Add()([d, layer_input])\n",
    "            return d\n",
    "\n",
    "        #解像度を2倍にするUpSampling\n",
    "        def deconv2d(layer_input):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input) #\n",
    "            u = Conv2D(120, kernel_size=3, strides=1, padding='same')(u) #\n",
    "            u = Activation('relu')(u)\n",
    "            return u\n",
    "\n",
    "        # Low resolution image input\n",
    "        img_lr = Input(shape=self.lr_shape)\n",
    "\n",
    "        # Pre-residual block\n",
    "        c1 = Conv2D(60, kernel_size=9, strides=1, padding='same')(img_lr) #\n",
    "        c1 = Activation('relu')(c1)\n",
    "\n",
    "        # Propogate through residual blocks\n",
    "        r = residual_block(c1, self.gf)\n",
    "        for _ in range(self.n_residual_blocks - 1):\n",
    "            r = residual_block(r, self.gf)\n",
    "\n",
    "        #去の残差ブロックと組み合わせる\n",
    "        # Post-residual block\n",
    "        c2 = Conv2D(60, kernel_size=3, strides=1, padding='same')(r) #\n",
    "        c2 = BatchNormalization(momentum=0.8)(c2)\n",
    "        c2 = Add()([c2, c1])\n",
    "\n",
    "        # Upsampling\n",
    "        u2 = deconv2d(c2)\n",
    "        #u2 = deconv2d(u1)\n",
    "\n",
    "        # Generate high resolution output\n",
    "        gen_hr = Conv2D(self.channels, kernel_size=9, strides=1, padding='same', activation='tanh')(u2)\n",
    "\n",
    "        return Model(img_lr, gen_hr)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
